{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:46:56.812640Z",
     "start_time": "2020-09-19T08:46:55.657996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Dot\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:00.685009Z",
     "start_time": "2020-09-19T08:47:00.656105Z"
    }
   },
   "outputs": [],
   "source": [
    "def maybe_download(filename, url, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "def collect_data(vocabulary_size=10000):\n",
    "    url = 'http://mattmahoney.net/dc/'\n",
    "    filename = maybe_download('text8.zip', url, 31344016)\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:21.650449Z",
     "start_time": "2020-09-19T08:47:17.245637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']\n",
      "[0, 0, 12, 6, 195, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocab_size)\n",
    "print(data[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:21.704688Z",
     "start_time": "2020-09-19T08:47:21.651561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK UNK as a term of UNK first used against early working class UNK including the UNK of the english revolution and the UNK UNK of the french revolution UNK the term is still used in a UNK way to UNK any act that used UNK means to UNK the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([reverse_dictionary[i] for i in data[:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:49.552731Z",
     "start_time": "2020-09-19T08:47:25.866444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61, 1], [940, 39], [133, 851], [698, 491], [20, 997], [178, 34], [819, 7], [844, 205], [304, 3], [799, 836]] [1, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "window_size = 3\n",
    "vector_dim = 30\n",
    "epochs = 200000\n",
    "\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)\n",
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")\n",
    "labels = np.array(labels, dtype = \"int32\")\n",
    "\n",
    "print(couples[:10], labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:49.977330Z",
     "start_time": "2020-09-19T08:47:49.553773Z"
    }
   },
   "outputs": [],
   "source": [
    "# create some input variables\n",
    "input_target = Input((1,), name = 'tagert')\n",
    "input_context = Input((1,), name = 'context')\n",
    "\n",
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:50.084348Z",
     "start_time": "2020-09-19T08:47:49.978745Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = Dot(axes = 1, normalize = True, name = 'cos_sim')([target, context])\n",
    "similarity = Reshape((1,))(similarity)\n",
    "\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(similarity)\n",
    "\n",
    "# create the primary training model\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(inputs=[input_target, input_context], outputs=similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:47:50.088522Z",
     "start_time": "2020-09-19T08:47:50.085290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "tagert (InputLayer)             [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 30)        30000       tagert[0][0]                     \n",
      "                                                                 context[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 30, 1)        0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 30, 1)        0           embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cos_sim (Dot)                   (None, 1, 1)         0           reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1)            0           cos_sim[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            2           reshape_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 30,002\n",
      "Trainable params: 30,002\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:48:46.438974Z",
     "start_time": "2020-09-19T08:48:46.418143Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "    \n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T08:51:16.872061Z",
     "start_time": "2020-09-19T08:48:58.640536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.6964299082756042\n",
      "Nearest to d: male, sound, because, jewish, how, give, all, david,\n",
      "Nearest to first: album, software, central, probably, frequently, roman, result, right,\n",
      "Nearest to these: jews, map, take, character, alexander, french, out, money,\n",
      "Nearest to a: five, could, actress, society, related, moved, enough, germany,\n",
      "Nearest to american: remained, special, mother, foreign, natural, forces, another, technology,\n",
      "Nearest to six: wife, technology, france, use, left, great, founded, current,\n",
      "Nearest to some: western, became, provided, similar, style, become, its, went,\n",
      "Nearest to to: scientific, bit, present, oil, but, network, lost, peace,\n",
      "Nearest to been: relationship, little, market, written, reference, field, takes, above,\n",
      "Nearest to united: me, origin, congress, america, go, name, currently, birth,\n",
      "Nearest to UNK: version, west, r, again, us, structure, their, appear,\n",
      "Nearest to world: memory, me, radio, as, position, it, century, africa,\n",
      "Nearest to one: move, case, museum, indian, reason, widely, founded, moon,\n",
      "Nearest to people: group, library, due, past, close, way, federal, leading,\n",
      "Nearest to known: al, months, final, earth, metal, country, came, new,\n",
      "Nearest to would: success, return, concept, territory, official, who, right, wide,\n",
      "Iteration 10, loss=0.6866909861564636\n",
      "Iteration 20, loss=0.694530189037323\n",
      "Iteration 30, loss=0.691773533821106\n",
      "Iteration 40, loss=0.6908280849456787\n",
      "Iteration 50, loss=0.675645649433136\n",
      "Iteration 60, loss=0.7026717066764832\n",
      "Iteration 70, loss=0.6918115019798279\n",
      "Iteration 80, loss=0.7015235424041748\n",
      "Iteration 90, loss=0.6970551013946533\n",
      "Iteration 100, loss=0.6914868354797363\n",
      "Nearest to d: sound, areas, all, charles, second, jewish, because, typically,\n",
      "Nearest to first: album, frequently, time, software, higher, without, remained, central,\n",
      "Nearest to these: take, jews, out, map, parts, does, based, length,\n",
      "Nearest to a: almost, five, title, claim, few, definition, actress, enough,\n",
      "Nearest to american: popular, force, technology, remained, jewish, mother, canada, age,\n",
      "Nearest to six: wife, france, zero, left, following, deaths, council, great,\n",
      "Nearest to some: western, its, similar, became, style, become, led, oil,\n",
      "Nearest to to: but, control, bit, peace, scientific, not, eight, great,\n",
      "Nearest to been: relationship, adopted, but, little, up, market, takes, henry,\n",
      "Nearest to united: me, birth, congress, rather, name, knowledge, currently, origin,\n",
      "Nearest to UNK: version, west, and, us, again, structure, r, thomas,\n",
      "Nearest to world: memory, catholic, century, africa, me, russian, roman, position,\n",
      "Nearest to one: move, founded, eight, page, reason, moon, alexander, you,\n",
      "Nearest to people: group, due, past, library, collection, federal, close, leading,\n",
      "Nearest to known: months, al, with, earth, came, final, country, relatively,\n",
      "Nearest to would: concept, success, who, official, soon, increased, security, treaty,\n",
      "Iteration 110, loss=0.6836318969726562\n",
      "Iteration 120, loss=0.6986114978790283\n",
      "Iteration 130, loss=0.6909856796264648\n",
      "Iteration 140, loss=0.695427417755127\n",
      "Iteration 150, loss=0.6815942525863647\n",
      "Iteration 160, loss=0.6866428852081299\n",
      "Iteration 170, loss=0.6917166709899902\n",
      "Iteration 180, loss=0.6912806034088135\n",
      "Iteration 190, loss=0.6862757205963135\n"
     ]
    }
   ],
   "source": [
    "bs = 32\n",
    "epochs = 200\n",
    "arr_1 = np.zeros((bs,1,))\n",
    "arr_2 = np.zeros((bs,1,))\n",
    "arr_3 = np.zeros((bs,1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1, size = bs)\n",
    "    arr_1[:,0,] = word_target[idx]\n",
    "    arr_2[:,0,] = word_context[idx]\n",
    "    arr_3[:,0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 10 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 100 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T09:00:31.673646Z",
     "start_time": "2020-09-19T08:57:18.957177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.7518122792243958\n",
      "Nearest to d: all, male, popular, j, sound, late, american, sent,\n",
      "Nearest to first: album, h, remained, frequently, central, industry, possible, growth,\n",
      "Nearest to these: almost, the, rules, take, french, asia, throughout, an,\n",
      "Nearest to a: was, the, to, with, of, then, is, by,\n",
      "Nearest to american: true, popular, in, around, body, jewish, used, another,\n",
      "Nearest to six: two, nine, zero, one, eight, seven, in, three,\n",
      "Nearest to some: however, similar, oil, its, led, influence, wrote, western,\n",
      "Nearest to to: the, need, a, claim, out, has, was, almost,\n",
      "Nearest to been: but, adopted, has, means, market, an, little, by,\n",
      "Nearest to united: me, church, origin, currently, rather, called, island, america,\n",
      "Nearest to UNK: us, version, surface, r, three, again, west, structure,\n",
      "Nearest to world: catholic, japanese, roman, last, included, take, god, memory,\n",
      "Nearest to one: nine, eight, seven, two, zero, six, in, published,\n",
      "Nearest to people: group, way, federal, due, collection, nature, species, unit,\n",
      "Nearest to known: well, earth, relatively, chinese, elected, notable, female, legal,\n",
      "Nearest to would: have, concept, open, types, still, growth, who, k,\n",
      "Iteration 320, loss=0.5696949362754822\n",
      "Iteration 640, loss=0.6595873236656189\n",
      "Iteration 960, loss=0.8509445786476135\n",
      "Iteration 1280, loss=0.6764041185379028\n",
      "Iteration 1600, loss=0.607216477394104\n",
      "Iteration 1920, loss=0.5097188949584961\n",
      "Iteration 2240, loss=0.5044620633125305\n",
      "Iteration 2560, loss=0.7420603632926941\n",
      "Iteration 2880, loss=0.7904027104377747\n",
      "Iteration 3200, loss=0.8085346221923828\n",
      "Nearest to d: male, j, sound, all, letters, popular, american, born,\n",
      "Nearest to first: album, h, possible, frequently, names, growth, central, industry,\n",
      "Nearest to these: almost, and, the, since, down, rules, was, even,\n",
      "Nearest to a: to, the, and, by, was, of, with, is,\n",
      "Nearest to american: true, another, age, body, jewish, anti, front, around,\n",
      "Nearest to six: nine, two, one, eight, three, zero, four, founded,\n",
      "Nearest to some: however, led, similar, western, directly, wrote, because, with,\n",
      "Nearest to to: the, a, of, and, was, in, is, by,\n",
      "Nearest to been: has, but, adopted, an, means, market, it, produced,\n",
      "Nearest to united: kingdom, me, almost, members, island, congress, church, these,\n",
      "Nearest to UNK: r, version, us, surface, again, these, structure, three,\n",
      "Nearest to world: catholic, last, roman, japanese, al, me, limited, memory,\n",
      "Nearest to one: eight, nine, two, seven, zero, six, four, three,\n",
      "Nearest to people: group, way, species, federal, past, across, small, collection,\n",
      "Nearest to known: well, relatively, earth, final, current, whether, style, chinese,\n",
      "Nearest to would: concept, open, types, growth, k, have, civil, still,\n",
      "Iteration 3520, loss=0.5301461815834045\n",
      "Iteration 3840, loss=0.798851490020752\n",
      "Iteration 4160, loss=0.8173229694366455\n",
      "Iteration 4480, loss=0.7557250261306763\n",
      "Iteration 4800, loss=0.6858553290367126\n",
      "Iteration 5120, loss=0.7313252091407776\n",
      "Iteration 5440, loss=0.8502487540245056\n",
      "Iteration 5760, loss=0.6220557689666748\n",
      "Iteration 6080, loss=0.7267853021621704\n"
     ]
    }
   ],
   "source": [
    "epochs = bs * 200\n",
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % (10 * bs) == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % (100 * bs) == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-19T09:45:33.722347Z",
     "start_time": "2020-09-19T09:45:33.687297Z"
    }
   },
   "outputs": [],
   "source": [
    "# output for tensorflow embedding visualization\n",
    "\n",
    "import pandas as pd\n",
    "weights = model.get_layer('embedding').weights[0].numpy()\n",
    "np.save('embedding_weights', weights)\n",
    "df_weights = pd.DataFrame(weights)\n",
    "df_weights.to_csv('embedding_weights.tsv', sep='\\t', index=False, header=False, float_format = '%8.6f')\n",
    "pd.DataFrame(dictionary.keys()).to_csv('embedding_meta.tsv', sep = '\\t', index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
